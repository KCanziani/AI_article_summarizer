{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Scraper and Summarizer using OpenAI\n",
    "\n",
    "This notebook automates the process of scraping news articles, summarizing them using OpenAI's GPT model, saving them to a CSV file, and displaying them in a user-friendly format. The process involves:\n",
    "1. Scraping articles from a website.\n",
    "2. Summarizing the content with OpenAI.\n",
    "3. Saving the articles and summaries into a CSV file.\n",
    "4. Displaying the data interactively.\n",
    "\n",
    "## Libraries and Configuration\n",
    "First, we import necessary libraries and set up configurations such as logging and the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 15:17:06,037 - INFO - Logging setup complete and OpenAI API key successfully set.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from config import OPENAI_API_KEY # Ensure this file contains your OpenAI API key\n",
    "import csv\n",
    "import pandas as pd\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"script_web.log\"), # Log to file\n",
    "        logging.StreamHandler()                # Log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "logging.info(\"Logging setup complete and OpenAI API key successfully set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Session with Retry Logic\n",
    "\n",
    "We create an HTTP session with retry logic to ensure the script handles intermittent connection issues gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 15:17:07,233 - INFO - HTTP session with retry logic initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize session with retry logic\n",
    "session = requests.Session()\n",
    "retries = HTTPAdapter(max_retries=5)\n",
    "\n",
    "session.mount('http://', retries)\n",
    "session.mount('https://', retries)\n",
    "\n",
    "logging.info(\"HTTP session with retry logic initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Below are the main functions used in this script:\n",
    "1. `scrape_articles(url)`: Scrapes articles from a given website URL.\n",
    "2. `summarize_with_openai(text)`: Summarizes the given text using OpenAI.\n",
    "3. `parse_article_date(date_str)`: Parses and standardizes article dates.\n",
    "4. `get_article_content(url)`: Fetches the main content of an article from its URL.\n",
    "5. `save_to_csv(articles, filename)`: Saves articles to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape articles from the website\n",
    "def scrape_articles(url):\n",
    "    \"\"\"\n",
    "    Scrapes articles from the provided website URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the website to scrape articles from.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains article data (title, link, date, type).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "        }\n",
    "        response = session.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_data = []\n",
    "\n",
    "        # Find the Featured section\n",
    "        featured_section = soup.find('section', class_='featured')\n",
    "        if featured_section:\n",
    "\n",
    "            # Find all featured article blocks with specific class\n",
    "            featured_blocks = featured_section.find_all('div', class_='cell blocks small-12 medium-3 large-3')\n",
    "\n",
    "            for block in featured_blocks:\n",
    "                try:\n",
    "                    # Get the image link and title\n",
    "                    link_element = block.find('a', class_='img-link')\n",
    "                    title_element = block.find('h3')\n",
    "                    date_div = block.find('div', class_='content')  # Date extraction\n",
    "\n",
    "                    if link_element and title_element:\n",
    "                        title = link_element['title'].strip()  # Title is in the link's title attribute\n",
    "                        link = link_element['href'].strip()   # Article URL\n",
    "                        date_str = date_div.text.strip().split('|')[0].strip() if date_div else 'No date available'\n",
    "                        date = parse_article_date(date_str) \n",
    "\n",
    "                        article_data.append({\n",
    "                            'Title': title,\n",
    "                            'Link': link,\n",
    "                            'Date': date,\n",
    "                            'Type': 'featured'\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing featured article: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Get regular articles\n",
    "        regular_articles = soup.find_all('article')\n",
    "\n",
    "        for article in regular_articles:\n",
    "            try:\n",
    "                title = article.find('h3').get_text(strip=True)\n",
    "                link = article.find('a')['href']\n",
    "                date_div = article.find('div', class_='content')\n",
    "                date_str = date_div.text.strip().split('|')[0].strip() if date_div else 'No date available'\n",
    "                date = parse_article_date(date_str)\n",
    "\n",
    "                # Check if this article is already in our list\n",
    "                if not any(a['Link'] == link for a in article_data):\n",
    "                    article_data.append({\n",
    "                        'Title': title,\n",
    "                        'Link': link,\n",
    "                        'Date': date,\n",
    "                        'Type': 'regular'\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing article: {e}\")\n",
    "                continue\n",
    "\n",
    "        return article_data\n",
    "    except RequestException as e:\n",
    "        logging.error(f\"Request error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to summarize article content using OpenAI\n",
    "def summarize_with_openai(text):\n",
    "    \"\"\"\n",
    "    Summarizes the given text using OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to summarize.\n",
    "\n",
    "    Returns:\n",
    "        str: Summary of the text or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_prompt = f\"Please provide a concise summary of this article: {text}\"\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes news articles.\"},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"OpenAI API error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to parse and standardize dates\n",
    "def parse_article_date(date_str):\n",
    "    \"\"\"\n",
    "    Converts a date string into a standardized date object.\n",
    "\n",
    "    Args:\n",
    "        date_str (str): Date string to parse.\n",
    "\n",
    "    Returns:\n",
    "        date: Parsed date object or None if parsing fails.\n",
    "    \"\"\"\n",
    "\n",
    "    formats = ['%d %B %Y', '%Y-%m-%d', '%B %d, %Y'] # Add more formats if needed\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt).date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    logging.warning(f\"Unrecognized date format: {date_str}\")\n",
    "    return None\n",
    "\n",
    "# Function to fetch the content of an article\n",
    "def get_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the main content of an article from its URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the article.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted article content or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = session.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content_container = soup.find('div', class_='article-content') or soup.find('article')\n",
    "\n",
    "        if content_container:\n",
    "            paragraphs = content_container.find_all('p')\n",
    "            return ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "        return \"\"\n",
    "    except RequestException as e:\n",
    "        logging.error(f\"Error fetching article content: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to save articles to CSV\n",
    "def save_to_csv(articles, filename):\n",
    "    \"\"\"\n",
    "    Saves a list of articles to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        articles (list): List of dictionaries containing article data.\n",
    "        filename (str): Name of the CSV file to save.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keys = ['Title', 'Date', 'Link', 'Type', 'Summary']\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(articles)\n",
    "        logging.info(f\"Articles saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving to CSV: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script\n",
    "In this section, we scrape today's news articles, summarize them, and save the results in a CSV file. The data is also displayed for easy viewing in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 15:18:19,094 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-23 15:18:29,500 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-23 15:18:38,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-23 15:18:46,318 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-23 15:18:46,327 - INFO - Articles saved to articles_2024-12-23.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_113d9 th {\n",
       "  background-color: #914048;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "  padding: 10px;\n",
       "}\n",
       "#T_113d9 td {\n",
       "  border: 1px solid #ddd;\n",
       "  padding: 8px;\n",
       "}\n",
       "#T_113d9 th {\n",
       "  border: 1px solid #ddd;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_113d9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_113d9_level0_col0\" class=\"col_heading level0 col0\" >Title</th>\n",
       "      <th id=\"T_113d9_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "      <th id=\"T_113d9_level0_col2\" class=\"col_heading level0 col2\" >Date</th>\n",
       "      <th id=\"T_113d9_level0_col3\" class=\"col_heading level0 col3\" >Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_113d9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_113d9_row0_col0\" class=\"data row0 col0\" >OpenAI funds $1 million study on AI and morality at Duke University</td>\n",
       "      <td id=\"T_113d9_row0_col1\" class=\"data row0 col1\" ><a href=\"https://www.artificialintelligence-news.com/news/openai-funds-1-million-study-on-ai-and-morality-at-duke-university/\" target=\"_blank\">https://www.artificialintelligence-news.com/news/openai-funds-1-million-study-on-ai-and-morality-at-duke-university/</a></td>\n",
       "      <td id=\"T_113d9_row0_col2\" class=\"data row0 col2\" >2024-12-23</td>\n",
       "      <td id=\"T_113d9_row0_col3\" class=\"data row0 col3\" >OpenAI has awarded a $1 million grant to Duke University's Moral Attitudes and Decisions Lab (MADLAB) for a project called \"Making Moral AI\". The research team, led by ethics professor Walter Sinnott-Armstrong and co-investigator Jana Schaich Borg, aims to develop a \"moral GPS\" that could guide ethical decision-making. The project will explore how AI might predict or influence moral judgments, such as ethical dilemmas in autonomous vehicles or business practices. However, the initiative raises questions about who determines the moral framework guiding these tools and whether AI should be trusted to make decisions with ethical implications. The grant will support the development of algorithms that predict human moral judgments in areas like medicine, law, and business.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_113d9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_113d9_row1_col0\" class=\"data row1 col0\" >Manhattan Project 2.0? US eyes AGI breakthrough in escalating China rivalry</td>\n",
       "      <td id=\"T_113d9_row1_col1\" class=\"data row1 col1\" ><a href=\"https://www.artificialintelligence-news.com/news/manhattan-project-2-0-us-eyes-agi-breakthrough-in-escalating-china-rivalry/\" target=\"_blank\">https://www.artificialintelligence-news.com/news/manhattan-project-2-0-us-eyes-agi-breakthrough-in-escalating-china-rivalry/</a></td>\n",
       "      <td id=\"T_113d9_row1_col2\" class=\"data row1 col2\" >2024-12-23</td>\n",
       "      <td id=\"T_113d9_row1_col3\" class=\"data row1 col3\" >The US-China Economic and Security Review Commission (USCC) has recommended a Manhattan Project-style initiative and restrictions on humanoid robots in its latest report to Congress. The report proposes a government-backed program to develop Artificial General Intelligence (AGI), AI systems that could match or exceed human cognitive abilities. The AGI initiative would provide multi-year contracts to leading AI companies, cloud providers, and data center operators, backed by the Defense Department’s highest priority, “DX Rating”. The report also suggests restricting imports of Chinese-made autonomous humanoid robots with advanced capabilities and targets energy infrastructure products with remote monitoring capabilities. The Commission also recommends stronger oversight of technology transfers and investment flows, and the creation of an Outbound Investment Office to prevent US capital and expertise from advancing China’s technological capabilities in sensitive sectors. The report also suggests eliminating China’s Permanent Normal Trade Relations status, which could reshape the technology supply chain and trade flows.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_113d9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_113d9_row2_col0\" class=\"data row2 col0\" >How blockchain, IoT, and AI are shaping the future of digital transformation</td>\n",
       "      <td id=\"T_113d9_row2_col1\" class=\"data row2 col1\" ><a href=\"https://www.artificialintelligence-news.com/news/how-blockchain-iot-and-ai-are-shaping-the-future-of-digital-transformation/\" target=\"_blank\">https://www.artificialintelligence-news.com/news/how-blockchain-iot-and-ai-are-shaping-the-future-of-digital-transformation/</a></td>\n",
       "      <td id=\"T_113d9_row2_col2\" class=\"data row2 col2\" >2024-12-23</td>\n",
       "      <td id=\"T_113d9_row2_col3\" class=\"data row2 col3\" >Blockchain, IoT, and AI are converging to redefine industries, according to David Palmer, chief product officer of Pairpoint by Vodafone. Blockchain has evolved from experimental concepts to practical tools, with applications in supply chain management and decentralized finance. IoT devices, expected to number around 30 billion worldwide by 2030, generate vast amounts of data that AI systems can use to provide actionable insights. Blockchain ensures the security and reliability of this data. Digital wallets, expected to grow from 4 billion today to 5.6 billion by 2030, are becoming a cornerstone of this ecosystem. The integration of finance into IoT devices allows for autonomous transactions, while decentralized physical infrastructure networks allow for shared resources. Governments are also exploring the potential of blockchain through Central Bank Digital Currencies and tokenized deposits. The convergence of these technologies could reshape industries and economies by 2030.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_113d9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_113d9_row3_col0\" class=\"data row3 col0\" >Ordnance Survey: Navigating the role of AI and ethical considerations in geospatial technology</td>\n",
       "      <td id=\"T_113d9_row3_col1\" class=\"data row3 col1\" ><a href=\"https://www.artificialintelligence-news.com/news/ordnance-survey-navigating-the-role-of-ai-and-ethical-considerations-in-geospatial-technology/\" target=\"_blank\">https://www.artificialintelligence-news.com/news/ordnance-survey-navigating-the-role-of-ai-and-ethical-considerations-in-geospatial-technology/</a></td>\n",
       "      <td id=\"T_113d9_row3_col2\" class=\"data row3 col2\" >2024-12-23</td>\n",
       "      <td id=\"T_113d9_row3_col3\" class=\"data row3 col3\" >Manish Jethwa, CTO at Ordnance Survey (OS), predicts significant advancements in artificial intelligence (AI) and machine learning (ML) in the coming year, particularly in the geospatial sector. He anticipates the integration of large language models with sophisticated agents to perform complex tasks and reduce barriers to interaction. This will make geospatial datasets more accessible and user-friendly. Jethwa also emphasizes the need for ethical considerations in AI development, including creating transparent, fair, and unbiased systems. He highlights the importance of workforce development and retraining to prepare employees for the impact of AI and digital transformation. Despite the potential of these advancements, challenges such as cultural resistance, change fatigue, and cybersecurity threats persist. Jethwa urges companies to develop comprehensive strategies to address these issues and to maintain a clear vision of future goals.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e5bfad1cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main execution logic\n",
    "url = \"https://www.artificialintelligence-news.com\"\n",
    "\n",
    "# Scrape articles\n",
    "articles = scrape_articles(url)\n",
    "\n",
    "# Use today's date for filtering\n",
    "today = datetime.now().date()\n",
    "\n",
    "filtered_articles = []\n",
    "for article in articles:\n",
    "    if article['Date'] == today:\n",
    "        content = get_article_content(article['Link'])\n",
    "        if content:\n",
    "            article['Summary'] = summarize_with_openai(content)\n",
    "            filtered_articles.append(article)\n",
    "\n",
    "# Save articles to a CSV file\n",
    "today_str = today.strftime('%Y-%m-%d')\n",
    "filename = f\"data/articles_{today_str}.csv\"\n",
    "save_to_csv(filtered_articles, filename)\n",
    "\n",
    "# Display articles in Jupyter Notebook with clickable links and formatted display\n",
    "if filtered_articles:\n",
    "    df = pd.DataFrame(filtered_articles)\n",
    "    df_display = df.drop(['Type'], axis=1)\n",
    "\n",
    "    # Function to make links clickable\n",
    "    def make_clickable(val):\n",
    "        return '<a href=\"{}\" target=\"_blank\">{}</a>'.format(val, val)\n",
    "\n",
    "    # Define the CSS styles\n",
    "    styles = [\n",
    "        # Header style\n",
    "        dict(selector=\"th\", props=[\n",
    "            (\"background-color\", \"#914048\"),  # Green background\n",
    "            (\"color\", \"white\"),               # White text\n",
    "            (\"font-weight\", \"bold\"),\n",
    "            (\"text-align\", \"center\"),\n",
    "            (\"padding\", \"10px\")\n",
    "        ]),\n",
    "        # Add grid to cells\n",
    "        dict(selector=\"td\", props=[\n",
    "            (\"border\", \"1px solid #ddd\"),\n",
    "            (\"padding\", \"8px\")\n",
    "        ]),\n",
    "        # Add grid to header cells\n",
    "        dict(selector=\"th\", props=[\n",
    "            (\"border\", \"1px solid #ddd\")\n",
    "        ])\n",
    "    ]\n",
    "    # Apply the formatting to the Link column\n",
    "    df_styled = df_display.style\\\n",
    "    .format({'Link': make_clickable})\\\n",
    "    .set_table_styles(styles)\n",
    "    # If you're working in a Jupyter notebook, display the styled DataFrame\n",
    "    display(df_styled)\n",
    "\n",
    "    filename_output = f\"results/articles_{today_str}.html\"\n",
    "    # If you're saving to HTML\n",
    "    df_styled.to_html(filename_output, escape=False)\n",
    "else:\n",
    "    print(\"No articles found for today.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
