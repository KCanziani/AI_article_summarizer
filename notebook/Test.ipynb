{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main entry point for AI News Scraper\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "from src.process_all_news import process_all_news\n",
    "from src.utils import setup_logging\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "EMAIL = os.getenv(\"EMAIL\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "IMAP_SERVER = os.getenv(\"IMAP_SERVER\")\n",
    "RECIPIENT_EMAIL = json.loads(os.getenv(\"RECIPIENT_EMAILS\", \"[]\"))\n",
    "if not RECIPIENT_EMAIL:\n",
    "    raise ValueError(\"No recipient emails configured\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "target_date = \"2025-03-16\"    \n",
    "# Process news\n",
    "a =process_all_news(RECIPIENT_EMAIL, target_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Web scraping functionality for AI News Scraper\n",
    "Contains functions to scrape articles from various news sources\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from src.utils import parse_article_date, setup_http_session\n",
    "\n",
    "# Initialize HTTP session\n",
    "session = setup_http_session()\n",
    "\n",
    "def scrape_articles_AI_news(url):\n",
    "    \"\"\"\n",
    "    Scrapes articles from the provided website URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the website to scrape articles from.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains article data (title, link, date).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "        }\n",
    "        response = session.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_data = []\n",
    "\n",
    "        # Find the Featured section\n",
    "        featured_section = soup.find('section', class_='featured')\n",
    "        if featured_section:\n",
    "\n",
    "            # Find all featured article blocks with specific class\n",
    "            featured_blocks = featured_section.find_all('div', class_='cell blocks small-12 medium-3 large-3')\n",
    "\n",
    "            for block in featured_blocks:\n",
    "                try:\n",
    "                    # Get the image link and title\n",
    "                    link_element = block.find('a', class_='img-link')\n",
    "                    title_element = block.find('h3')\n",
    "                    date_div = block.find('div', class_='content')  # Date extraction\n",
    "\n",
    "                    if link_element and title_element:\n",
    "                        title = link_element['title'].strip()  # Title is in the link's title attribute\n",
    "                        link = link_element['href'].strip()   # Article URL\n",
    "                        date_str = date_div.text.strip().split('|')[0].strip() if date_div else 'No date available'\n",
    "                        date = parse_article_date(date_str) \n",
    "\n",
    "                        article_data.append({\n",
    "                            'Title': title,\n",
    "                            'Link': link,\n",
    "                            'Date': date,\n",
    "                            'Source': \"AI News\"\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing featured article: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Get regular articles\n",
    "        regular_articles = soup.find_all('article')\n",
    "\n",
    "        for article in regular_articles:\n",
    "            try:\n",
    "                title = article.find('h3').get_text(strip=True)\n",
    "                link = article.find('a')['href']\n",
    "                date_div = article.find('div', class_='content')\n",
    "                date_str = date_div.text.strip().split('|')[0].strip() if date_div else 'No date available'\n",
    "                date = parse_article_date(date_str)\n",
    "\n",
    "                # Check if this article is already in our list\n",
    "                if not any(a['Link'] == link for a in article_data):\n",
    "                    article_data.append({\n",
    "                        'Title': title,\n",
    "                        'Link': link,\n",
    "                        'Date': date,\n",
    "                        'Source': \"AI News\"\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing article: {e}\")\n",
    "                continue\n",
    "\n",
    "        return article_data\n",
    "    except RequestException as e:\n",
    "        logging.error(f\"Request error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to fetch the content of an article\n",
    "def get_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the main content of an article from its URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the article.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted article content or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = session.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content_container = soup.find('div', class_='article-content') or soup.find('article')\n",
    "\n",
    "        if content_container:\n",
    "            paragraphs = content_container.find_all('p')\n",
    "            return ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "        return \"\"\n",
    "    except RequestException as e:\n",
    "        logging.error(f\"Error fetching article content: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "def scrape_mit_articles(url):\n",
    "    \"\"\"\n",
    "    Scrapes articles from MIT AI News\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_data = []\n",
    "\n",
    "        # Find all article elements with the correct class\n",
    "        articles = soup.find_all('article', class_='term-page--news-article--item')\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_element = article.find('h3', class_='term-page--news-article--item--title')\n",
    "                title = title_element.find('a').get_text(strip=True) if title_element else None\n",
    "\n",
    "                # Extract link\n",
    "                link_element = article.find('a', class_='term-page--news-article--item--title--link')\n",
    "                link = link_element['href'] if link_element else None\n",
    "                if link and not link.startswith('http'):\n",
    "                    link = f\"https://news.mit.edu{link}\"\n",
    "\n",
    "                # Extract date and convert to date object\n",
    "                date_element = article.find('time')\n",
    "                date_str = date_element['datetime'] if date_element else None\n",
    "                if date_str:\n",
    "                    date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00')).date()\n",
    "                else:\n",
    "                    date_obj = None\n",
    "\n",
    "                # # Extract summary\n",
    "                # summary_element = article.find('p', class_='term-page--news-article--item--dek')\n",
    "                # summary = summary_element.get_text(strip=True) if summary_element else None\n",
    "\n",
    "                if all([title, link]):  # Add article if at least title and link are present\n",
    "                    article_data.append({\n",
    "                        'Title': title,\n",
    "                        'Link': link,\n",
    "                        'Date': date_obj,\n",
    "                        'Source': \"MIT News\"\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing MIT article: {e}\")\n",
    "                continue\n",
    "\n",
    "        return article_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in scraping MIT: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_mit_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of a MIT News article\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Try to find the main article content\n",
    "        content_container = (\n",
    "            soup.find('div', class_='news-article--content--body') or  # Try specific content class first\n",
    "            soup.find('article') or                                    # Then try main article tag\n",
    "            soup.find('main')                                         # Finally try main content area\n",
    "        )\n",
    "        \n",
    "        if content_container:\n",
    "            # Get all paragraphs\n",
    "            paragraphs = content_container.find_all('p')\n",
    "            \n",
    "            # Clean and join the text\n",
    "            content = ' '.join([\n",
    "                p.get_text(strip=True) \n",
    "                for p in paragraphs \n",
    "                if p.get_text(strip=True) and \n",
    "                   'Previous image' not in p.get_text() and\n",
    "                   'Next image' not in p.get_text()\n",
    "            ])\n",
    "            \n",
    "            # Additional cleaning\n",
    "            content = content.replace('Previous imageNext image', '')\n",
    "            \n",
    "            if len(content) > 100:  # Basic check to ensure we got meaningful content\n",
    "                return content\n",
    "                \n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logging.err\n",
    "\n",
    "def scrape_stanford_articles(url):\n",
    "    \"\"\"\n",
    "    Scrapes articles from Stanford AI News\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_data = []\n",
    "\n",
    "        news_container = soup.find('div', {'data-component': 'topic-subtopic-listing'})\n",
    "        if news_container:\n",
    "            import json\n",
    "            props = json.loads(news_container['data-hydration-props'])\n",
    "            articles = props.get('data', [])\n",
    "            \n",
    "            for article in articles:\n",
    "                try:\n",
    "                    # Convert timestamp to date object immediately\n",
    "                    if article.get('date'):\n",
    "                        date_obj = datetime.fromtimestamp(article.get('date')/1000).date()\n",
    "                    else:\n",
    "                        date_obj = None\n",
    "\n",
    "                    #summary = article.get('description', [''])[0] if isinstance(article.get('description'), list) else article.get('description')\n",
    "                    article_data.append({\n",
    "                        'Title': article.get('title'),\n",
    "                        'Link': article.get('liveUrl'),\n",
    "                        'Date': date_obj,\n",
    "                        'Source': \"Stanford News\"\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing Stanford article: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return article_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in scraping Stanford: {e}\")\n",
    "        return []\n",
    "        \n",
    "def get_stanford_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of a Stanford News article\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find the main article content\n",
    "        content_container = soup.find('div', class_='su-page-content') or soup.find('article')\n",
    "        \n",
    "        if content_container:\n",
    "            paragraphs = content_container.find_all('p')\n",
    "            return ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching Stanford article content: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'AI reveals insights into the flow of Antarctic ice',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/03/antarctic-ice-flow-research-ai',\n",
       "  'Date': datetime.date(2025, 3, 12),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'New book explores how relationships shape learning in age of AI',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/03/isabelle-hau-love-to-learn-book-relationships-learning',\n",
       "  'Date': datetime.date(2025, 3, 11),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'Stanford engineers help prepare Air Force test pilots for autonomous technology advances',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/02/stanford-engineers-help-prepare-air-force-test-pilots-advances-autonomous-technologies',\n",
       "  'Date': datetime.date(2025, 2, 26),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'Summit explores role of human-centered AI in the learning ecosystem',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/02/the-future-is-already-here-ai-and-education-in-2025',\n",
       "  'Date': datetime.date(2025, 2, 26),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'Carlos Guestrin to lead Stanford AI Lab as it joins forces with Stanford HAI',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/02/carlos-guestrin-to-lead-stanford-ai-lab-as-it-joins-forces-with-stanford-hai',\n",
       "  'Date': datetime.date(2025, 2, 19),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'Generative AI tool marks a milestone in biology',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/02/generative-ai-tool-marks-a-milestone-in-biology-and-accelerates-the-future-of-life-sciences',\n",
       "  'Date': datetime.date(2025, 2, 18),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'How disruptive is DeepSeek? Stanford HAI faculty discuss Chinaâ€™s new model',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/02/how-disruptive-is-deepseek',\n",
       "  'Date': datetime.date(2025, 2, 12),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'New study takes novel approach to mitigating bias in LLMs',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/02/bias-in-large-language-models-and-who-should-be-held-accountable',\n",
       "  'Date': datetime.date(2025, 2, 12),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'Study suggests physicians make better decisions with help of AI chatbots',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/02/study-suggests-physician-s-medical-decisions-benefit-from-chatbot',\n",
       "  'Date': datetime.date(2025, 2, 6),\n",
       "  'Source': 'Stanford News'},\n",
       " {'Title': 'Legal Design Lab receives Gates Foundation grant to address justice gap with AI',\n",
       "  'Link': 'https://news.stanford.edu/stories/2025/01/empowering-legal-aid',\n",
       "  'Date': datetime.date(2025, 1, 26),\n",
       "  'Source': 'Stanford News'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://news.stanford.edu/artificial-intelligence\"\n",
    "\n",
    "scrape_stanford_articles(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
