{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Scraper and Summarizer using OpenAI\n",
    "\n",
    "This notebook automates the process of scraping news articles, summarizing them using OpenAI's GPT model, saving them to a CSV file, and displaying them in a user-friendly format. The process involves:\n",
    "1. Scraping articles from a website.\n",
    "2. Summarizing the content with OpenAI.\n",
    "3. Saving the articles and summaries into a CSV file.\n",
    "4. Displaying the data interactively.\n",
    "\n",
    "## Libraries and Configuration\n",
    "First, we import necessary libraries and set up configurations such as logging and the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "from config import OPENAI_API_KEY, EMAIL, PASSWORD, RECIPIENT_EMAIL # Ensure this file contains your OpenAI API key\n",
    "import csv\n",
    "import pandas as pd\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.exceptions import RequestException\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:01:34,262 - INFO - Logging setup complete and OpenAI API key successfully set.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"script_web.log\"), # Log to file\n",
    "        logging.StreamHandler()                # Log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "logging.info(\"Logging setup complete and OpenAI API key successfully set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Session with Retry Logic\n",
    "\n",
    "We create an HTTP session with retry logic to ensure the script handles intermittent connection issues gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:01:35,650 - INFO - HTTP session with retry logic initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize session with retry logic\n",
    "session = requests.Session()\n",
    "retries = HTTPAdapter(max_retries=5)\n",
    "\n",
    "session.mount('http://', retries)\n",
    "session.mount('https://', retries)\n",
    "\n",
    "logging.info(\"HTTP session with retry logic initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Below are the main functions used in this script:\n",
    "1. `scrape_articles(url)`: Scrapes articles from a given website URL.\n",
    "2. `summarize_with_openai(text)`: Summarizes the given text using OpenAI.\n",
    "3. `parse_article_date(date_str)`: Parses and standardizes article dates.\n",
    "4. `get_article_content(url)`: Fetches the main content of an article from its URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape articles from the website\n",
    "def scrape_articles(url):\n",
    "    \"\"\"\n",
    "    Scrapes articles from the provided website URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the website to scrape articles from.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains article data (title, link, date, type).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "        }\n",
    "        response = session.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_data = []\n",
    "\n",
    "        # Find the Featured section\n",
    "        featured_section = soup.find('section', class_='featured')\n",
    "        if featured_section:\n",
    "\n",
    "            # Find all featured article blocks with specific class\n",
    "            featured_blocks = featured_section.find_all('div', class_='cell blocks small-12 medium-3 large-3')\n",
    "\n",
    "            for block in featured_blocks:\n",
    "                try:\n",
    "                    # Get the image link and title\n",
    "                    link_element = block.find('a', class_='img-link')\n",
    "                    title_element = block.find('h3')\n",
    "                    date_div = block.find('div', class_='content')  # Date extraction\n",
    "\n",
    "                    if link_element and title_element:\n",
    "                        title = link_element['title'].strip()  # Title is in the link's title attribute\n",
    "                        link = link_element['href'].strip()   # Article URL\n",
    "                        date_str = date_div.text.strip().split('|')[0].strip() if date_div else 'No date available'\n",
    "                        date = parse_article_date(date_str) \n",
    "\n",
    "                        article_data.append({\n",
    "                            'Title': title,\n",
    "                            'Link': link,\n",
    "                            'Date': date,\n",
    "                            'Type': 'featured'\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing featured article: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Get regular articles\n",
    "        regular_articles = soup.find_all('article')\n",
    "\n",
    "        for article in regular_articles:\n",
    "            try:\n",
    "                title = article.find('h3').get_text(strip=True)\n",
    "                link = article.find('a')['href']\n",
    "                date_div = article.find('div', class_='content')\n",
    "                date_str = date_div.text.strip().split('|')[0].strip() if date_div else 'No date available'\n",
    "                date = parse_article_date(date_str)\n",
    "\n",
    "                # Check if this article is already in our list\n",
    "                if not any(a['Link'] == link for a in article_data):\n",
    "                    article_data.append({\n",
    "                        'Title': title,\n",
    "                        'Link': link,\n",
    "                        'Date': date,\n",
    "                        'Type': 'regular'\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing article: {e}\")\n",
    "                continue\n",
    "\n",
    "        return article_data\n",
    "    except RequestException as e:\n",
    "        logging.error(f\"Request error: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_mit_articles(url):\n",
    "    \"\"\"\n",
    "    Scrapes articles from MIT AI News\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_data = []\n",
    "\n",
    "        # Find all article elements with the correct class\n",
    "        articles = soup.find_all('article', class_='term-page--news-article--item')\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_element = article.find('h3', class_='term-page--news-article--item--title')\n",
    "                title = title_element.find('a').get_text(strip=True) if title_element else None\n",
    "\n",
    "                # Extract link\n",
    "                link_element = article.find('a', class_='term-page--news-article--item--title--link')\n",
    "                link = link_element['href'] if link_element else None\n",
    "                if link and not link.startswith('http'):\n",
    "                    link = f\"https://news.mit.edu{link}\"\n",
    "\n",
    "                # Extract date and convert to date object\n",
    "                date_element = article.find('time')\n",
    "                date_str = date_element['datetime'] if date_element else None\n",
    "                if date_str:\n",
    "                    date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00')).date()\n",
    "                else:\n",
    "                    date_obj = None\n",
    "\n",
    "                # Extract summary\n",
    "                summary_element = article.find('p', class_='term-page--news-article--item--dek')\n",
    "                summary = summary_element.get_text(strip=True) if summary_element else None\n",
    "\n",
    "                if all([title, link]):  # Add article if at least title and link are present\n",
    "                    article_data.append({\n",
    "                        'Title': title,\n",
    "                        'Link': link,\n",
    "                        'Date': date_obj,\n",
    "                        'Summary': summary\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing MIT article: {e}\")\n",
    "                continue\n",
    "\n",
    "        return article_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in scraping MIT: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_stanford_articles(url):\n",
    "    \"\"\"\n",
    "    Scrapes articles from Stanford AI News\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_data = []\n",
    "\n",
    "        news_container = soup.find('div', {'data-component': 'topic-subtopic-listing'})\n",
    "        if news_container:\n",
    "            import json\n",
    "            props = json.loads(news_container['data-hydration-props'])\n",
    "            articles = props.get('data', [])\n",
    "            \n",
    "            for article in articles:\n",
    "                try:\n",
    "                    # Convert timestamp to date object immediately\n",
    "                    if article.get('date'):\n",
    "                        date_obj = datetime.fromtimestamp(article.get('date')/1000).date()\n",
    "                    else:\n",
    "                        date_obj = None\n",
    "\n",
    "                    article_data.append({\n",
    "                        'Title': article.get('title'),\n",
    "                        'Link': article.get('liveUrl'),\n",
    "                        'Date': date_obj,\n",
    "                        'Summary': article.get('description', [''])[0] if isinstance(article.get('description'), list) else article.get('description')\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing Stanford article: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return article_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in scraping Stanford: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to summarize article content using OpenAI\n",
    "def summarize_with_openai(text):\n",
    "    \"\"\"\n",
    "    Summarizes the given text using OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to summarize.\n",
    "\n",
    "    Returns:\n",
    "        str: Summary of the text or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_prompt = f\"Please provide a concise summary of this article: {text}\"\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes news articles.\"},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"OpenAI API error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to parse and standardize dates\n",
    "def parse_article_date(date_str):\n",
    "    \"\"\"\n",
    "    Converts a date string into a standardized date object.\n",
    "\n",
    "    Args:\n",
    "        date_str (str): Date string to parse.\n",
    "\n",
    "    Returns:\n",
    "        date: Parsed date object or None if parsing fails.\n",
    "    \"\"\"\n",
    "\n",
    "    formats = ['%d %B %Y', '%Y-%m-%d', '%B %d, %Y'] # Add more formats if needed\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt).date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    logging.warning(f\"Unrecognized date format: {date_str}\")\n",
    "    return None\n",
    "\n",
    "# Function to fetch the content of an article\n",
    "def get_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the main content of an article from its URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the article.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted article content or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = session.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content_container = soup.find('div', class_='article-content') or soup.find('article')\n",
    "\n",
    "        if content_container:\n",
    "            paragraphs = content_container.find_all('p')\n",
    "            return ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "        return \"\"\n",
    "    except RequestException as e:\n",
    "        logging.error(f\"Error fetching article content: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_mit_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of a MIT News article\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Try to find the main article content\n",
    "        content_container = (\n",
    "            soup.find('div', class_='news-article--content--body') or  # Try specific content class first\n",
    "            soup.find('article') or                                    # Then try main article tag\n",
    "            soup.find('main')                                         # Finally try main content area\n",
    "        )\n",
    "        \n",
    "        if content_container:\n",
    "            # Get all paragraphs\n",
    "            paragraphs = content_container.find_all('p')\n",
    "            \n",
    "            # Clean and join the text\n",
    "            content = ' '.join([\n",
    "                p.get_text(strip=True) \n",
    "                for p in paragraphs \n",
    "                if p.get_text(strip=True) and \n",
    "                   'Previous image' not in p.get_text() and\n",
    "                   'Next image' not in p.get_text()\n",
    "            ])\n",
    "            \n",
    "            # Additional cleaning\n",
    "            content = content.replace('Previous imageNext image', '')\n",
    "            \n",
    "            if len(content) > 100:  # Basic check to ensure we got meaningful content\n",
    "                return content\n",
    "                \n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logging.err\n",
    "        \n",
    "def get_stanford_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of a Stanford News article\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, timeout=30, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find the main article content\n",
    "        content_container = soup.find('div', class_='su-page-content') or soup.find('article')\n",
    "        \n",
    "        if content_container:\n",
    "            paragraphs = content_container.find_all('p')\n",
    "            return ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching Stanford article content: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script\n",
    "In this section, we scrape today's news articles, summarize them, and save the results in a CSV file. The data is also displayed for easy viewing in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_sources():\n",
    "    \"\"\"\n",
    "    Scrapes articles from all three sources and combines them\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    \n",
    "    # Scrape AI News\n",
    "    ai_news_url = \"https://www.artificialintelligence-news.com\"\n",
    "    ai_articles = scrape_articles(ai_news_url)\n",
    "    for article in ai_articles:\n",
    "        article['Source'] = 'AI News'\n",
    "        all_articles.append(article)\n",
    "    \n",
    "    # Scrape MIT News\n",
    "    mit_url = \"https://news.mit.edu/topic/artificial-intelligence2\"\n",
    "    mit_articles = scrape_mit_articles(mit_url)\n",
    "    for article in mit_articles:\n",
    "        article['Source'] = 'MIT News'\n",
    "        all_articles.append(article)\n",
    "    \n",
    "    # Scrape Stanford News\n",
    "    stanford_url = \"https://news.stanford.edu/artificial-intelligence\"\n",
    "    stanford_articles = scrape_stanford_articles(stanford_url)\n",
    "    for article in stanford_articles:\n",
    "        article['Source'] = 'Stanford News'\n",
    "        all_articles.append(article)\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "def safe_str(value):\n",
    "    \"\"\"Convert any value to string safely.\"\"\"\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    return str(value)\n",
    "\n",
    "# Function to save articles to CSV\n",
    "def save_to_csv(articles, filename):\n",
    "    \"\"\"\n",
    "    Saves a list of articles to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        articles (list): List of dictionaries containing article data.\n",
    "        filename (str): Name of the CSV file to save.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keys = ['Title', 'Date', 'Link', 'Type', 'Summary', 'Source']  # Added 'Source'\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=keys, extrasaction='ignore')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(articles)\n",
    "        logging.info(f\"Articles saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving to CSV: {e}\")\n",
    "        raise  # Re-raise the exception to handle it in the calling function\n",
    "\n",
    "def send_combined_email_report(articles, date_str, recipients):\n",
    "    \"\"\"\n",
    "    Sends an email with articles from all sources\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create HTML content with source grouping\n",
    "        html_content = f\"\"\"\n",
    "        <html>\n",
    "            <head>\n",
    "                <style>\n",
    "                    table {{ \n",
    "                        border-collapse: collapse; \n",
    "                        width: 100%; \n",
    "                        margin-bottom: 30px;\n",
    "                    }}\n",
    "                    th, td {{ \n",
    "                        padding: 8px; \n",
    "                        text-align: left; \n",
    "                        border: 1px solid #ddd; \n",
    "                    }}\n",
    "                    th {{ \n",
    "                        background-color: #914048; \n",
    "                        color: white; \n",
    "                    }}\n",
    "                    .source-header {{\n",
    "                        background-color: #f5f5f5;\n",
    "                        padding: 10px;\n",
    "                        margin: 20px 0 10px 0;\n",
    "                        font-size: 1.2em;\n",
    "                        font-weight: bold;\n",
    "                    }}\n",
    "                    a {{ \n",
    "                        color: #0066cc; \n",
    "                        text-decoration: none; \n",
    "                    }}\n",
    "                    a:hover {{ \n",
    "                        text-decoration: underline; \n",
    "                    }}\n",
    "                </style>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h2>Weekly AI News Summary {date_str}</h2>\n",
    "        \"\"\"\n",
    "\n",
    "        # Group articles by source\n",
    "        sources = ['AI News', 'MIT News', 'Stanford News']\n",
    "        for source in sources:\n",
    "            source_articles = [a for a in articles if a.get('Source') == source]\n",
    "            if source_articles:\n",
    "                html_content += f\"\"\"\n",
    "                    <div class=\"source-header\">{source}</div>\n",
    "                    <table>\n",
    "                        <tr>\n",
    "                            <th>Title</th>\n",
    "                            <th>Summary</th>\n",
    "                            <th>Link</th>\n",
    "                        </tr>\n",
    "                \"\"\"\n",
    "                \n",
    "                for article in source_articles:\n",
    "                    html_content += f\"\"\"\n",
    "                        <tr>\n",
    "                            <td>{safe_str(article.get('Title'))}</td>\n",
    "                            <td>{safe_str(article.get('Summary'))}</td>\n",
    "                            <td><a href=\"{safe_str(article.get('Link'))}\">{safe_str(article.get('Link'))}</a></td>\n",
    "                        </tr>\n",
    "                    \"\"\"\n",
    "                \n",
    "                html_content += \"</table>\"\n",
    "\n",
    "        html_content += \"\"\"\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        # Create SMTP session and send email\n",
    "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
    "            server.login(EMAIL, PASSWORD)\n",
    "            \n",
    "            # Handle single recipient or list of recipients\n",
    "            if isinstance(recipients, str):\n",
    "                recipients = [recipients]\n",
    "            \n",
    "            for recipient in recipients:\n",
    "                try:\n",
    "                    msg = MIMEMultipart('alternative')\n",
    "                    msg['Subject'] = f'Weekly AI News Summary {date_str}'\n",
    "                    msg['From'] = EMAIL\n",
    "                    msg['To'] = recipient\n",
    "                    \n",
    "                    html_part = MIMEText(html_content, 'html', 'utf-8')\n",
    "                    msg.attach(html_part)\n",
    "                    \n",
    "                    server.send_message(msg)\n",
    "                    logging.info(f\"Email sent successfully to {recipient}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error sending email to {recipient}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in email sending process: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_all_news(recipients, target_date=None):\n",
    "    \"\"\"\n",
    "    Process news from all sources for the past week and send combined email\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set target date range\n",
    "        if target_date:\n",
    "            try:\n",
    "                end_date = datetime.strptime(target_date, '%Y-%m-%d').date()\n",
    "            except ValueError:\n",
    "                raise ValueError(\"Date must be in format 'YYYY-MM-DD'\")\n",
    "        else:\n",
    "            end_date = datetime.now().date()\n",
    "            \n",
    "        # Calculate start date (7 days before end date)\n",
    "        start_date = end_date - timedelta(days=7)\n",
    "        date_str = f\"{start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\"\n",
    "            \n",
    "        logging.info(f\"Processing news for date range: {date_str}\")\n",
    "        \n",
    "        # Get all articles\n",
    "        all_articles = scrape_all_sources()\n",
    "        logging.info(f\"Total articles found: {len(all_articles)}\")\n",
    "        \n",
    "        # Filter and process articles\n",
    "        processed_articles = []\n",
    "        for article in all_articles:\n",
    "            article_date = article.get('Date')\n",
    "            \n",
    "            # Check if article date is within the week range\n",
    "            if article_date and start_date <= article_date <= end_date:\n",
    "                # Get content and create summary based on source\n",
    "                if article['Source'] == 'AI News':\n",
    "                    content = get_article_content(article['Link'])\n",
    "                elif article['Source'] == 'MIT News':\n",
    "                    content = get_mit_article_content(article['Link'])\n",
    "                elif article['Source'] == 'Stanford News':\n",
    "                    content = get_stanford_article_content(article['Link'])\n",
    "                else:\n",
    "                    content = \"\"\n",
    "\n",
    "                if content:\n",
    "                    article['Summary'] = summarize_with_openai(content)\n",
    "                processed_articles.append(article)\n",
    "        \n",
    "        logging.info(f\"Found {len(processed_articles)} articles for date range {date_str}\")\n",
    "        \n",
    "        if processed_articles:\n",
    "            # Save to CSV\n",
    "            save_path = f\"data/articles_week_{end_date.strftime('%Y-%m-%d')}.csv\"\n",
    "            try:\n",
    "                save_to_csv(processed_articles, save_path)\n",
    "                \n",
    "                # Send email\n",
    "                send_combined_email_report(processed_articles, date_str, recipients)\n",
    "                logging.info(\"Combined email sent successfully!\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in saving or sending: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            logging.info(f\"No articles found for date range: {date_str}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in process_all_news: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:06:04,533 - INFO - Processing news for date: 2025-01-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 MIT articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:06:07,835 - INFO - Total articles found: 59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 Stanford articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:06:17,578 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 15:06:28,339 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 15:06:35,414 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 15:06:44,788 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-27 15:06:44,793 - INFO - Found 4 articles for date 2025-01-16\n",
      "2025-01-27 15:06:44,795 - INFO - Articles saved to data/combined_articles_2025-01-16.csv\n",
      "2025-01-27 15:06:47,119 - INFO - Email sent successfully to karina.canziani@gmail.com\n",
      "2025-01-27 15:06:47,292 - INFO - Combined email sent successfully!\n"
     ]
    }
   ],
   "source": [
    "process_all_news(RECIPIENT_EMAIL, target_date=\"2025-01-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # For manual testing\n",
    "    process_all_news(RECIPIENT_EMAIL)\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Starting scheduler...\")\n",
    "        scheduler.start()\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        logging.info(\"Scheduler stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
