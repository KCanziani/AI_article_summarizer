{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming your src folder is one level up from where the notebook is running\n",
    "# Adjust the path as needed\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# Now try importing\n",
    "from src.summarizer import summarize_with_openai\n",
    "\n",
    "class YoutubeChannelSubtitles:\n",
    "    def __init__(self, api_key):\n",
    "        self.youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        \n",
    "    def get_channel_id(self, channel_name):\n",
    "        \"\"\"Gets the channel ID from the channel name\"\"\"\n",
    "        request = self.youtube.search().list(\n",
    "            q=channel_name,\n",
    "            type='channel',\n",
    "            part='id',\n",
    "            maxResults=1\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        if response['items']:\n",
    "            return response['items'][0]['id']['channelId']\n",
    "        return None\n",
    "\n",
    "    def get_recent_videos(self, channel_id, max_results=50, days_back=7):\n",
    "        \"\"\"Gets videos from the channel published in the last X days\"\"\"\n",
    "        # Request more videos than needed to ensure we have enough after filtering\n",
    "        request = self.youtube.search().list(\n",
    "            channelId=channel_id,\n",
    "            order='date',  # Sort by date\n",
    "            part='snippet',\n",
    "            maxResults=max_results,  # Request more to filter afterward\n",
    "            type='video'\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        videos = []\n",
    "        # Calculate cutoff date (7 days ago from now)\n",
    "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "        \n",
    "        for item in response['items']:\n",
    "            # Convert published date string to datetime object\n",
    "            published_at = datetime.strptime(\n",
    "                item['snippet']['publishedAt'], \n",
    "                '%Y-%m-%dT%H:%M:%SZ'\n",
    "            )\n",
    "            \n",
    "            # Only include videos published after the cutoff date\n",
    "            if published_at >= cutoff_date:\n",
    "                video = {\n",
    "                    'title': item['snippet']['title'],\n",
    "                    'video_id': item['id']['videoId'],\n",
    "                    'published_at': item['snippet']['publishedAt']\n",
    "                }\n",
    "                videos.append(video)\n",
    "        \n",
    "        return videos\n",
    "\n",
    "    def download_subtitles(self, video_id, languages=['es', 'en'], output_dir='subtitles'):\n",
    "        \"\"\"Downloads subtitles for a video in the specified languages\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "            results = {}\n",
    "            \n",
    "            for language in languages:\n",
    "                try:\n",
    "                    transcript = transcript_list.find_transcript([language])\n",
    "                    subtitles = transcript.fetch()\n",
    "                    \n",
    "                    # Save in JSON format\n",
    "                    filename = f'{output_dir}/{video_id}_{language}.json'\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(subtitles, f, ensure_ascii=False, indent=2)\n",
    "                    \n",
    "                    results[language] = 'Success'\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    results[language] = f'Failed: {str(e)}'\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f'Failed to get subtitles: {str(e)}'\n",
    "\n",
    "    def process_channel(self, channel_name, max_videos=50, languages=['es', 'en'], days_back=7):\n",
    "        \"\"\"Processes videos from a channel within the specified time frame\"\"\"\n",
    "        # Get channel ID\n",
    "        channel_id = self.get_channel_id(channel_name)\n",
    "        if not channel_id:\n",
    "            return f\"Channel not found: {channel_name}\"\n",
    "\n",
    "        # Get recent videos from the last X days\n",
    "        videos = self.get_recent_videos(channel_id, max_videos, days_back)\n",
    "        \n",
    "        if not videos:\n",
    "            return f\"No videos found in the last {days_back} days for channel: {channel_name}\"\n",
    "        \n",
    "        results = []\n",
    "        for video in videos:\n",
    "            result = {\n",
    "                'title': video['title'],\n",
    "                'video_id': video['video_id'],\n",
    "                'published_at': video['published_at'],\n",
    "                'subtitles': self.download_subtitles(video['video_id'], languages)\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        # Save results to a log file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        with open(f'results_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def process_multiple_channels_to_csv(self, channel_names, max_videos=5, days_back=7):\n",
    "        \"\"\"\n",
    "        Process videos from multiple channels and combine them into a single CSV\n",
    "        \n",
    "        Args:\n",
    "            channel_names (list): List of channel names to process\n",
    "            max_videos (int): Maximum videos to fetch per channel\n",
    "            days_back (int): Only include videos from the last X days\n",
    "            \n",
    "        Returns:\n",
    "            str: Filename of the created CSV\n",
    "        \"\"\"\n",
    "        # List to store data from all channels\n",
    "        all_data = []\n",
    "        \n",
    "        # Process each channel\n",
    "        for channel_name in channel_names:\n",
    "            print(f\"Processing channel: {channel_name}\")\n",
    "            \n",
    "            # Get channel ID\n",
    "            channel_id = self.get_channel_id(channel_name)\n",
    "            if not channel_id:\n",
    "                print(f\"Channel not found: {channel_name}\")\n",
    "                continue\n",
    "\n",
    "            # Get videos from the last X days\n",
    "            videos = self.get_recent_videos(channel_id, max_videos, days_back)\n",
    "            \n",
    "            if not videos:\n",
    "                print(f\"No videos found in the last {days_back} days for channel: {channel_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Process each video\n",
    "            for video in videos:\n",
    "                video_id = video['video_id']\n",
    "                title = video['title']\n",
    "                date = datetime.strptime(video['published_at'], '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d')\n",
    "                video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                \n",
    "                try:\n",
    "                    # Get subtitles\n",
    "                    transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "                    \n",
    "                    # Try manual subtitles first\n",
    "                    try:\n",
    "                        transcript = transcript_list.find_manually_created_transcript()\n",
    "                    except:\n",
    "                        # If no manual subtitles, try with any available language\n",
    "                        transcript = transcript_list.find_transcript(['en', 'es'])\n",
    "                    \n",
    "                    subtitles = transcript.fetch()\n",
    "                    language = transcript.language\n",
    "                    \n",
    "                    # Get text from each subtitle entry\n",
    "                    subtitle_texts = [entry['text'] for entry in subtitles]\n",
    "                    \n",
    "                    # Clean special characters from each text segment\n",
    "                    cleaned_texts = []\n",
    "                    for text in subtitle_texts:\n",
    "                        # Replace newlines with spaces\n",
    "                        cleaned_text = text.replace('\\n', ' ')\n",
    "                        # Replace escaped backslashes\n",
    "                        cleaned_text = cleaned_text.replace('\\\\', '')\n",
    "                        # Replace multiple spaces with a single space\n",
    "                        import re\n",
    "                        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "                        cleaned_texts.append(cleaned_text)\n",
    "                    \n",
    "                    # Join all cleaned text segments with spaces\n",
    "                    full_text = ' '.join(cleaned_texts).strip()\n",
    "                    \n",
    "                    summary = \"not summary yet\" #summarize_with_openai(full_text)\n",
    "\n",
    "                    # Add to data list with channel name\n",
    "                    all_data.append({\n",
    "                        'Title': title,\n",
    "                        'Date': date,\n",
    "                        'Link': video_url,\n",
    "                        'Summary': summary,\n",
    "                        'Source': channel_name,  # Add channel name\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing video {video_id}: {str(e)}\")\n",
    "        \n",
    "        # If no data was collected\n",
    "        if not all_data:\n",
    "            return \"No videos found for any of the specified channels\"\n",
    "            \n",
    "        # Create DataFrame and save as CSV\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df = df[[ 'Title', 'Date', 'Link', 'Summary', 'Source']]\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        csv_filename = f'multi_channel_subtitles_{timestamp}.csv'\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing channel: @la_inteligencia_artificial\n",
      "Processing channel: @dotcsv\n",
      "No videos found in the last 7 days for channel: @dotcsv\n",
      "Processing channel: @gustavo-entrala\n",
      "CSV generado:                                                Title        Date  \\\n",
      "0         ¬°BRUTAL lo nuevo de OpenAI y es GRATIS! üî•ü§Ø  2025-03-26   \n",
      "1  ü§ñ Consiguen reconectar el cerebro de una perso...  2025-03-26   \n",
      "2  ü§ñ Lo primero y m√°s importante es validar la id...  2025-03-25   \n",
      "3  ü§ñ Existe una burbuja de valorizaciones de empr...  2025-03-25   \n",
      "4  ü§ñ Ninguna sociedad est√° preparada para el camb...  2025-03-25   \n",
      "5  El Chip Cu√°ntico MAJORANA: el mayor INVENTO de...  2025-03-21   \n",
      "\n",
      "                                          Link          Summary  \\\n",
      "0  https://www.youtube.com/watch?v=aOqgMPjJBf8  not summary yet   \n",
      "1  https://www.youtube.com/watch?v=w9EUvKfsHPI  not summary yet   \n",
      "2  https://www.youtube.com/watch?v=8WBS9-caOUA  not summary yet   \n",
      "3  https://www.youtube.com/watch?v=CES1h_yZ83A  not summary yet   \n",
      "4  https://www.youtube.com/watch?v=Ad76pb-bb7I  not summary yet   \n",
      "5  https://www.youtube.com/watch?v=iCmgjF1p9CU  not summary yet   \n",
      "\n",
      "                        Source  \n",
      "0  @la_inteligencia_artificial  \n",
      "1  @la_inteligencia_artificial  \n",
      "2  @la_inteligencia_artificial  \n",
      "3  @la_inteligencia_artificial  \n",
      "4  @la_inteligencia_artificial  \n",
      "5             @gustavo-entrala  \n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = 'AIzaSyDKckSI_0asveDz4lHuU_KMrmmU9zy0-18'\n",
    "    yt = YoutubeChannelSubtitles(API_KEY)\n",
    "    \n",
    "    csv_file = yt.process_multiple_channels_to_csv(\n",
    "        channel_names=[\"@la_inteligencia_artificial\", \"@dotcsv\", \"@gustavo-entrala\"],\n",
    "        max_videos=5,\n",
    "        days_back=7\n",
    "    )\n",
    "    \n",
    "    print(f\"CSV generado: {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬°BRUTAL lo nuevo de OpenAI y es GRATIS! üî•ü§Ø</td>\n",
       "      <td>2025-03-26</td>\n",
       "      <td>https://www.youtube.com/watch?v=aOqgMPjJBf8</td>\n",
       "      <td>not summary yet</td>\n",
       "      <td>@la_inteligencia_artificial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ü§ñ Consiguen reconectar el cerebro de una perso...</td>\n",
       "      <td>2025-03-26</td>\n",
       "      <td>https://www.youtube.com/watch?v=w9EUvKfsHPI</td>\n",
       "      <td>not summary yet</td>\n",
       "      <td>@la_inteligencia_artificial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ü§ñ Lo primero y m√°s importante es validar la id...</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>https://www.youtube.com/watch?v=8WBS9-caOUA</td>\n",
       "      <td>not summary yet</td>\n",
       "      <td>@la_inteligencia_artificial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ü§ñ Existe una burbuja de valorizaciones de empr...</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>https://www.youtube.com/watch?v=CES1h_yZ83A</td>\n",
       "      <td>not summary yet</td>\n",
       "      <td>@la_inteligencia_artificial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ü§ñ Ninguna sociedad est√° preparada para el camb...</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>https://www.youtube.com/watch?v=Ad76pb-bb7I</td>\n",
       "      <td>not summary yet</td>\n",
       "      <td>@la_inteligencia_artificial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>El Chip Cu√°ntico MAJORANA: el mayor INVENTO de...</td>\n",
       "      <td>2025-03-21</td>\n",
       "      <td>https://www.youtube.com/watch?v=iCmgjF1p9CU</td>\n",
       "      <td>not summary yet</td>\n",
       "      <td>@gustavo-entrala</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Date  \\\n",
       "0         ¬°BRUTAL lo nuevo de OpenAI y es GRATIS! üî•ü§Ø  2025-03-26   \n",
       "1  ü§ñ Consiguen reconectar el cerebro de una perso...  2025-03-26   \n",
       "2  ü§ñ Lo primero y m√°s importante es validar la id...  2025-03-25   \n",
       "3  ü§ñ Existe una burbuja de valorizaciones de empr...  2025-03-25   \n",
       "4  ü§ñ Ninguna sociedad est√° preparada para el camb...  2025-03-25   \n",
       "5  El Chip Cu√°ntico MAJORANA: el mayor INVENTO de...  2025-03-21   \n",
       "\n",
       "                                          Link          Summary  \\\n",
       "0  https://www.youtube.com/watch?v=aOqgMPjJBf8  not summary yet   \n",
       "1  https://www.youtube.com/watch?v=w9EUvKfsHPI  not summary yet   \n",
       "2  https://www.youtube.com/watch?v=8WBS9-caOUA  not summary yet   \n",
       "3  https://www.youtube.com/watch?v=CES1h_yZ83A  not summary yet   \n",
       "4  https://www.youtube.com/watch?v=Ad76pb-bb7I  not summary yet   \n",
       "5  https://www.youtube.com/watch?v=iCmgjF1p9CU  not summary yet   \n",
       "\n",
       "                        Source  \n",
       "0  @la_inteligencia_artificial  \n",
       "1  @la_inteligencia_artificial  \n",
       "2  @la_inteligencia_artificial  \n",
       "3  @la_inteligencia_artificial  \n",
       "4  @la_inteligencia_artificial  \n",
       "5             @gustavo-entrala  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Web scraping functionality for AI News Scraper\n",
    "Contains functions to scrape articles from various news sources\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from src.summarizer import summarize_with_openai\n",
    "from config.config import AI_NEWS_URL, MIT_NEWS_URL, STANFORD_NEWS_URL, YOUTUBE_API_KEY, YOUTUBE_CHANNELS\n",
    "from src.email_sender import send_combined_email_report\n",
    "from src.scraper import scrape_articles_AI_news, get_article_content, scrape_mit_articles, get_mit_article_content, scrape_stanford_articles, get_stanford_article_content\n",
    "from src.youtube_scraper import process_youtube_channels\n",
    "import csv\n",
    "\n",
    "# Get logger\n",
    "logger = logging.getLogger('ai_news_scraper.processor')\n",
    "\n",
    "def save_to_csv(articles, filename) -> None:\n",
    "    \"\"\"\n",
    "    Save articles to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        articles (List[Dict]): List of articles to save\n",
    "        filename (str): Path to save the CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keys = ['Title', 'Date', 'Link','Summary', 'Source']\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=keys, extrasaction='ignore')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(articles)\n",
    "        logger.info(f\"Articles saved to CSV: {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving to CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_to_html(articles, date_str, filename):\n",
    "    \"\"\"\n",
    "    Save articles to a styled HTML file.\n",
    "    \n",
    "    Args:\n",
    "        articles (list): List of processed articles\n",
    "        date_str (str): Date range string for the title\n",
    "        filename (str): Path to save the HTML file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        \n",
    "        if not articles:\n",
    "            logging.warning(\"No articles to save to HTML\")\n",
    "            return\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(articles)\n",
    "        \n",
    "        # Define function to make links clickable\n",
    "        def make_clickable(val):\n",
    "            return f'<a href=\"{val}\" target=\"_blank\">{val}</a>'\n",
    "\n",
    "        # Define CSS styles\n",
    "        styles = [\n",
    "            # Header style\n",
    "            dict(selector=\"th\", props=[\n",
    "                (\"background-color\", \"#914048\"),\n",
    "                (\"color\", \"white\"),\n",
    "                (\"font-weight\", \"bold\"),\n",
    "                (\"text-align\", \"center\"),\n",
    "                (\"padding\", \"10px\"),\n",
    "                (\"border\", \"1px solid #ddd\")\n",
    "            ]),\n",
    "            # Cell style\n",
    "            dict(selector=\"td\", props=[\n",
    "                (\"border\", \"1px solid #ddd\"),\n",
    "                (\"padding\", \"8px\"),\n",
    "                (\"text-align\", \"left\")\n",
    "            ]),\n",
    "            # Table style\n",
    "            dict(selector=\"\", props=[\n",
    "                (\"border-collapse\", \"collapse\"),\n",
    "                (\"width\", \"100%\"),\n",
    "                (\"margin\", \"20px 0\"),\n",
    "                (\"font-family\", \"Arial, sans-serif\")\n",
    "            ]),\n",
    "            # Source group style\n",
    "            dict(selector=\".source-header\", props=[\n",
    "                (\"background-color\", \"#f5f5f5\"),\n",
    "                (\"padding\", \"10px\"),\n",
    "                (\"margin\", \"20px 0 10px 0\"),\n",
    "                (\"font-size\", \"1.2em\"),\n",
    "                (\"font-weight\", \"bold\")\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "        # Create HTML template\n",
    "        html_template = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>AI News Summary - {date_str}</title>\n",
    "            <style>\n",
    "                body {{\n",
    "                    font-family: Arial, sans-serif;\n",
    "                    margin: 20px;\n",
    "                    background-color: #f8f9fa;\n",
    "                }}\n",
    "                .container {{\n",
    "                    max-width: 1200px;\n",
    "                    margin: 0 auto;\n",
    "                    background-color: white;\n",
    "                    padding: 20px;\n",
    "                    box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
    "                }}\n",
    "                h1 {{\n",
    "                    color: #333;\n",
    "                    text-align: center;\n",
    "                    padding-bottom: 20px;\n",
    "                    border-bottom: 2px solid #914048;\n",
    "                }}\n",
    "                .source-section {{\n",
    "                    margin-top: 30px;\n",
    "                }}\n",
    "                .source-header {{\n",
    "                    background-color: #f5f5f5;\n",
    "                    padding: 10px;\n",
    "                    margin: 20px 0 10px 0;\n",
    "                    font-size: 1.2em;\n",
    "                    font-weight: bold;\n",
    "                    border-left: 4px solid #914048;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>AI News Summary - {date_str}</h1>\n",
    "        \"\"\"\n",
    "\n",
    "        # Group articles by source\n",
    "        sources = sorted(set(article['Source'] for article in articles))\n",
    "        \n",
    "        for source in sources:\n",
    "            source_articles = [a for a in articles if a['Source'] == source]\n",
    "            if source_articles:\n",
    "                # Create DataFrame for this source\n",
    "                df_source = pd.DataFrame(source_articles)\n",
    "                \n",
    "                # Select and reorder columns\n",
    "                columns_to_display = ['Title', 'Date', 'Link', 'Summary']\n",
    "                df_display = df_source[columns_to_display].copy()\n",
    "                \n",
    "                # Format the date column\n",
    "                df_display['Date'] = df_display['Date'].apply(lambda x: x.strftime('%Y-%m-%d') if pd.notnull(x) else '')\n",
    "                \n",
    "                # Style the DataFrame\n",
    "                df_styled = df_display.style\\\n",
    "                    .format({'Link': make_clickable})\\\n",
    "                    .set_table_styles(styles)\n",
    "                \n",
    "                # Add source section to HTML\n",
    "                html_template += f\"\"\"\n",
    "                    <div class=\"source-section\">\n",
    "                        <div class=\"source-header\">{source}</div>\n",
    "                        {df_styled.to_html(escape=False)}\n",
    "                    </div>\n",
    "                \"\"\"\n",
    "\n",
    "        html_template += \"\"\"\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        # Save to file\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_template)\n",
    "            \n",
    "        logger.info(f\"Articles saved to HTML: {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to HTML: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_all_news(recipients, target_date=None):\n",
    "    \"\"\"\n",
    "    Process news from all sources and send combined email.\n",
    "    \n",
    "    Args:\n",
    "        recipients (str or list): Email recipient(s)\n",
    "        target_date (str, optional): Target date in YYYY-MM-DD format\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set date range\n",
    "        end_date = (datetime.strptime(target_date, '%Y-%m-%d').date() \n",
    "                   if target_date else datetime.now().date())\n",
    "        start_date = end_date - timedelta(days=7)\n",
    "        date_str = f\"{start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "        logger.info(f\"Processing news for date range: {date_str}\")\n",
    "\n",
    "        # Get and process articles\n",
    "        all_articles = []\n",
    "        source_counts = {}  # Para llevar la cuenta de art√≠culos por fuente\n",
    "\n",
    "        source_configs = [\n",
    "            ('AI News', scrape_articles_AI_news, get_article_content),\n",
    "            ('MIT News', scrape_mit_articles, get_mit_article_content),\n",
    "            ('Stanford News', scrape_stanford_articles, get_stanford_article_content)\n",
    "        ]\n",
    "\n",
    "        # Collect articles from all sources\n",
    "        for source_name, scraper_func, content_func in source_configs:\n",
    "            try:\n",
    "                logger.info(f\"Processing source: {source_name}\")\n",
    "                url = globals()[f\"{source_name.upper().replace(' ', '_')}_URL\"]\n",
    "                articles = scraper_func(url)\n",
    "                \n",
    "                source_articles = []  # Art√≠culos para esta fuente\n",
    "\n",
    "                # Process each article\n",
    "                for article in articles:\n",
    "                    article_date = article.get('Date')\n",
    "                    \n",
    "                    # Check if article is within date range\n",
    "                    if article_date and start_date <= article_date <= end_date:\n",
    "                        # Get and summarize content\n",
    "                        content = content_func(article['Link'])\n",
    "                        if content:\n",
    "                            article['Summary'] = \"not summary yet\" #summarize_with_openai(content)\n",
    "                        article['Source'] = source_name\n",
    "                        all_articles.append(article)\n",
    "                        source_articles.append(article)\n",
    "\n",
    "                source_counts[source_name] = len(source_articles)\n",
    "                logger.info(f\"Found {len(source_articles)} articles from {source_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {source_name}: {e}\")\n",
    "                source_counts[source_name] = 0\n",
    "                continue\n",
    "\n",
    "        # Process YouTube channels\n",
    "        try:\n",
    "            if YOUTUBE_API_KEY and YOUTUBE_CHANNELS:\n",
    "                youtube_articles = process_youtube_channels(\n",
    "                    YOUTUBE_API_KEY, \n",
    "                    YOUTUBE_CHANNELS,\n",
    "                    max_videos=10, \n",
    "                    days_back=(end_date - start_date).days\n",
    "                )\n",
    "                \n",
    "                # Filter by date range (should already be filtered, but double-check)\n",
    "                youtube_articles = [\n",
    "                    article for article in youtube_articles \n",
    "                    if article.get('Date') and start_date <= article.get('Date') <= end_date\n",
    "                ]\n",
    "                \n",
    "                all_articles.append(youtube_articles)\n",
    "                source_counts['YouTube'] = len(youtube_articles)\n",
    "                logger.info(f\"Added {len(youtube_articles)} YouTube videos to articles\")\n",
    "            else:\n",
    "                logger.info(\"YouTube processing skipped: API key or channels not configured\")\n",
    "                source_counts['YouTube'] = 0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing YouTube channels: {e}\")\n",
    "            source_counts['YouTube'] = 0\n",
    "        \n",
    "        print(youtube_articles)\n",
    "        # Log total counts\n",
    "        logger.info(f\"Total articles collected: {len(all_articles)}\")\n",
    "        for source, count in source_counts.items():\n",
    "            logger.info(f\"  - {source}: {count} articles\")\n",
    "\n",
    "        # if all_articles:\n",
    "        #     # Create date string for filenames\n",
    "        #     end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "        #     # Save to CSV\n",
    "        #     csv_path = f\"data/articles_week_{end_date_str}.csv\"\n",
    "        #     save_to_csv(all_articles, csv_path)\n",
    "            \n",
    "        #     # Save to HTML\n",
    "        #     html_path = f\"results/articles_week_{end_date_str}.html\"\n",
    "        #     save_to_html(all_articles, date_str, html_path)\n",
    "            \n",
    "        #     # Send email\n",
    "        #     send_combined_email_report(all_articles, date_str, recipients)\n",
    "        #     logger.info(\"Articles processed, saved, and email sent successfully!\")\n",
    "        # else:\n",
    "        #     logger.info(f\"No articles found for date range: {date_str}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_all_news: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting channel ID for '@la_inteligencia_artificial': 'str' object has no attribute 'search'\n",
      "Channel not found: @la_inteligencia_artificial\n",
      "Error getting channel ID for '@dotcsv': 'str' object has no attribute 'search'\n",
      "Channel not found: @dotcsv\n",
      "Error getting channel ID for '@gustavo-entrala': 'str' object has no attribute 'search'\n",
      "Channel not found: @gustavo-entrala\n",
      "No videos found for any of the specified channels\n",
      "Error processing YouTube channels: 'str' object has no attribute 'get'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No videos found for any of the specified channels\n"
     ]
    }
   ],
   "source": [
    "EMAIL = os.getenv(\"EMAIL\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "IMAP_SERVER = os.getenv(\"IMAP_SERVER\")\n",
    "RECIPIENT_EMAIL = json.loads(os.getenv(\"RECIPIENT_EMAILS\", \"[]\"))\n",
    "if not RECIPIENT_EMAIL:\n",
    "    raise ValueError(\"No recipient emails configured\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "target_date = '2025-03-16'\n",
    "    \n",
    "# Process news\n",
    "process_all_news(RECIPIENT_EMAIL, target_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Get logger\n",
    "logger = logging.getLogger('ai_news_scraper.youtube')\n",
    "\n",
    "# Resto de tus funciones de YouTube tal como las definiste...\n",
    "def build_youtube_client(api_key):\n",
    "    \"\"\"\n",
    "    Construye y devuelve un cliente de la API de YouTube.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Clave de API de YouTube\n",
    "        \n",
    "    Returns:\n",
    "        object: Cliente de la API de YouTube\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not api_key or not isinstance(api_key, str):\n",
    "            logger.error(f\"Invalid YouTube API key: {api_key}\")\n",
    "            return None\n",
    "\n",
    "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        logger.info(\"YouTube API client created successfully\")\n",
    "        return youtube\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create YouTube API client: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_channel_id(youtube, channel_name):\n",
    "    \"\"\"\n",
    "    Obtiene el ID del canal a partir del nombre del canal.\n",
    "    \n",
    "    Args:\n",
    "        youtube (object): Cliente de la API de YouTube\n",
    "        channel_name (str): Nombre del canal\n",
    "        \n",
    "    Returns:\n",
    "        str: ID del canal o None si no se encuentra\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            q=channel_name,\n",
    "            type='channel',\n",
    "            part='id',\n",
    "            maxResults=1\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        if response['items']:\n",
    "            channel_id = response['items'][0]['id']['channelId']\n",
    "            logger.info(f\"Found channel ID for '{channel_name}': {channel_id}\")\n",
    "            return channel_id\n",
    "        \n",
    "        logger.warning(f\"No channel found for name '{channel_name}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting channel ID for '{channel_name}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_recent_videos(youtube, channel_id, max_results=50, days_back=7):\n",
    "    \"\"\"\n",
    "    Obtiene videos recientes del canal publicados en los √∫ltimos X d√≠as.\n",
    "    \n",
    "    Args:\n",
    "        youtube (object): Cliente de la API de YouTube\n",
    "        channel_id (str): ID del canal\n",
    "        max_results (int): N√∫mero m√°ximo de resultados\n",
    "        days_back (int): Cuando contar los 7 dias\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de videos con informaci√≥n\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            channelId=channel_id,\n",
    "            order='date',\n",
    "            part='snippet',\n",
    "            maxResults=max_results,\n",
    "            type='video'\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        videos = []\n",
    "        cutoff_date = datetime.now() - timedelta(days=7)\n",
    "        \n",
    "        for item in response['items']:\n",
    "            published_at = datetime.strptime(\n",
    "                item['snippet']['publishedAt'], \n",
    "                '%Y-%m-%dT%H:%M:%SZ'\n",
    "            )\n",
    "            \n",
    "            if published_at >= cutoff_date:\n",
    "                video = {\n",
    "                    'title': item['snippet']['title'],\n",
    "                    'video_id': item['id']['videoId'],\n",
    "                    'published_at': item['snippet']['publishedAt']\n",
    "                }\n",
    "                videos.append(video)\n",
    "        \n",
    "        logger.info(f\"Found {len(videos)} videos from the last {days_back} days for channel {channel_id}\")\n",
    "        return videos\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting videos for channel {channel_id}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def download_subtitles(video_id, languages=['es', 'en'], output_dir='subtitles'):\n",
    "    \"\"\"\n",
    "    Descarga subt√≠tulos para un video en los idiomas especificados.\n",
    "    \n",
    "    Args:\n",
    "        video_id (str): ID del video\n",
    "        languages (list): Lista de c√≥digos de idioma\n",
    "        output_dir (str): Directorio de salida\n",
    "        \n",
    "    Returns:\n",
    "        dict: Resultados para cada idioma\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crear directorio si no existe\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            logger.info(f\"Created directory: {output_dir}\")\n",
    "\n",
    "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "        results = {}\n",
    "        \n",
    "        for language in languages:\n",
    "            try:\n",
    "                transcript = transcript_list.find_transcript([language])\n",
    "                subtitles = transcript.fetch()\n",
    "                \n",
    "                # Guardar en formato JSON\n",
    "                filename = f'{output_dir}/{video_id}_{language}.json'\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(subtitles, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                results[language] = 'Success'\n",
    "                logger.info(f\"Downloaded {language} subtitles for video {video_id}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[language] = f'Failed: {str(e)}'\n",
    "                logger.warning(f\"Failed to download {language} subtitles for video {video_id}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to get any subtitles for video {video_id}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return {'error': error_msg}\n",
    "\n",
    "\n",
    "def process_channel(youtube, channel_name, max_videos=50, languages=['es', 'en'], days_back=7):\n",
    "    \"\"\"\n",
    "    Procesa videos de un canal dentro del per√≠odo de tiempo especificado.\n",
    "    \n",
    "    Args:\n",
    "        youtube (object): Cliente de la API de YouTube\n",
    "        channel_name (str): Nombre del canal\n",
    "        max_videos (int): N√∫mero m√°ximo de videos\n",
    "        languages (list): Lista de idiomas para los subt√≠tulos\n",
    "        days_back (int): D√≠as hacia atr√°s para filtrar\n",
    "        \n",
    "    Returns:\n",
    "        list: Resultados del procesamiento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processing channel: {channel_name}\")\n",
    "        \n",
    "        # Obtener ID del canal\n",
    "        channel_id = get_channel_id(youtube, channel_name)\n",
    "        if not channel_id:\n",
    "            error_msg = f\"Channel not found: {channel_name}\"\n",
    "            logger.warning(error_msg)\n",
    "            return error_msg\n",
    "\n",
    "        # Obtener videos recientes\n",
    "        videos = get_recent_videos(youtube, channel_id, max_videos, days_back)\n",
    "        \n",
    "        if not videos:\n",
    "            msg = f\"No videos found in the last {days_back} days for channel: {channel_name}\"\n",
    "            logger.info(msg)\n",
    "            return msg\n",
    "        \n",
    "        results = []\n",
    "        for video in videos:\n",
    "            try:\n",
    "                result = {\n",
    "                    'title': video['title'],\n",
    "                    'video_id': video['video_id'],\n",
    "                    'published_at': video['published_at'],\n",
    "                    'subtitles': download_subtitles(video['video_id'], languages)\n",
    "                }\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing video {video['video_id']}: {str(e)}\")\n",
    "        \n",
    "        # Guardar resultados en un archivo JSON\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        results_file = f'results_{timestamp}.json'\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved processing results to {results_file}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing channel {channel_name}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(traceback.format_exc())\n",
    "        return error_msg\n",
    "\n",
    "\n",
    "def clean_subtitle_text(text):\n",
    "    \"\"\"\n",
    "    Limpia el texto de los subt√≠tulos.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a limpiar\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto limpio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import re\n",
    "        \n",
    "        # Reemplazar saltos de l√≠nea con espacios\n",
    "        cleaned_text = text.replace('\\n', ' ')\n",
    "        # Reemplazar barras invertidas escapadas\n",
    "        cleaned_text = cleaned_text.replace('\\\\', '')\n",
    "        # Reemplazar m√∫ltiples espacios con uno solo\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        \n",
    "        return cleaned_text.strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning subtitle text: {str(e)}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_video_transcript(video_id):\n",
    "    \"\"\"\n",
    "    Obtiene la transcripci√≥n completa de un video.\n",
    "    \n",
    "    Args:\n",
    "        video_id (str): ID del video\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (texto completo, idioma) o (None, None) si falla\n",
    "    \"\"\"\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "        \n",
    "        # Intentar con subt√≠tulos manuales primero\n",
    "        try:\n",
    "            transcript = transcript_list.find_manually_created_transcript()\n",
    "        except:\n",
    "            # Si no hay subt√≠tulos manuales, probar con cualquier idioma disponible\n",
    "            transcript = transcript_list.find_transcript(['en', 'es'])\n",
    "        \n",
    "        subtitles = transcript.fetch()\n",
    "        language = transcript.language\n",
    "        \n",
    "        # Obtener texto de cada entrada de subt√≠tulos\n",
    "        subtitle_texts = [entry['text'] for entry in subtitles]\n",
    "        \n",
    "        # Limpiar cada segmento de texto\n",
    "        cleaned_texts = [clean_subtitle_text(text) for text in subtitle_texts]\n",
    "        \n",
    "        # Unir todos los segmentos de texto limpios con espacios\n",
    "        full_text = ' '.join(cleaned_texts).strip()\n",
    "        \n",
    "        logger.info(f\"Successfully retrieved transcript for video {video_id} in {language}\")\n",
    "        return full_text, language\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get transcript for video {video_id}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def process_youtube_channels(api_key, channel_names, max_videos=5, days_back=7):\n",
    "    \"\"\"\n",
    "    Procesa videos de m√∫ltiples canales y los combina en un solo CSV.\n",
    "    \n",
    "    Args:\n",
    "        youtube (object): Cliente de la API de YouTube\n",
    "        channel_names (list): Lista de nombres de canales\n",
    "        max_videos (int): M√°ximo de videos por canal\n",
    "        days_back (int): Solo incluir videos de los √∫ltimos X d√≠as\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame o str: DataFrame con los datos o mensaje de error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processing {len(channel_names)} channels: {', '.join(channel_names)}\")\n",
    "        \n",
    "        youtube = build_youtube_client(api_key)\n",
    "\n",
    "        # Lista para almacenar datos de todos los canales\n",
    "        all_data = []\n",
    "        \n",
    "        # Procesar cada canal\n",
    "        for channel_name in channel_names:\n",
    "            try:\n",
    "                logger.info(f\"Processing channel: {channel_name}\")\n",
    "                \n",
    "                # Obtener ID del canal\n",
    "                channel_id = get_channel_id(youtube, channel_name)\n",
    "                if not channel_id:\n",
    "                    logger.warning(f\"Channel not found: {channel_name}\")\n",
    "                    continue\n",
    "\n",
    "                # Obtener videos de los √∫ltimos X d√≠as\n",
    "                videos = get_recent_videos(youtube, channel_id, max_videos, days_back)\n",
    "                \n",
    "                if not videos:\n",
    "                    logger.info(f\"No videos found in the last 7 days for channel: {channel_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar cada video\n",
    "                for video in videos:\n",
    "                    try:\n",
    "                        video_id = video['video_id']\n",
    "                        title = video['title']\n",
    "                        date = datetime.strptime(video['published_at'], '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d')\n",
    "                        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                        \n",
    "                        # Obtener transcripci√≥n\n",
    "                        full_text, language = get_video_transcript(video_id)\n",
    "                        \n",
    "                        if full_text:\n",
    "                            # Generar resumen (descomentado cuando se implemente)\n",
    "                            try:\n",
    "                                summary = \"not summary yet\"  # summarize_with_openai(full_text)\n",
    "                                \n",
    "                                # A√±adir a la lista de datos con el nombre del canal\n",
    "                                all_data.append({\n",
    "                                    'Title': title,\n",
    "                                    'Date': date,\n",
    "                                    'Link': video_url,\n",
    "                                    'Summary': summary,\n",
    "                                    'Source': channel_name,\n",
    "                                    'Language': language\n",
    "                                })\n",
    "                                \n",
    "                                logger.info(f\"Successfully processed video: {title}\")\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"Error generating summary for video {video_id}: {str(e)}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing video {video.get('video_id', 'unknown')}: {str(e)}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing channel {channel_name}: {str(e)}\")\n",
    "        \n",
    "        # Si no se recopil√≥ ning√∫n dato\n",
    "        if not all_data:\n",
    "            msg = \"No videos found for any of the specified channels\"\n",
    "            logger.warning(msg)\n",
    "            return msg\n",
    "                \n",
    "        # # Crear DataFrame y guardar como CSV\n",
    "        # df = pd.DataFrame(all_data)\n",
    "        # df = df[['Title', 'Date', 'Link', 'Summary', 'Source', 'Language']]\n",
    "        \n",
    "        # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        # csv_filename = f'multi_channel_subtitles_{timestamp}.csv'\n",
    "        # df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        # logger.info(f\"Successfully saved data to {csv_filename}. Processed {len(all_data)} videos.\")\n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing multiple channels: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(traceback.format_exc())\n",
    "        return error_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting channel ID for '@la_inteligencia_artificial': <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=%40la_inteligencia_artificial&type=channel&part=id&maxResults=1&key=AIzaSyDKckSI_0asveDz4lHuU_KMrmmU9zy0-18&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Channel not found: @la_inteligencia_artificial\n",
      "Error getting channel ID for '@dotcsv': <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=%40dotcsv&type=channel&part=id&maxResults=1&key=AIzaSyDKckSI_0asveDz4lHuU_KMrmmU9zy0-18&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Channel not found: @dotcsv\n",
      "Error getting channel ID for '@gustavo-entrala': <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=%40gustavo-entrala&type=channel&part=id&maxResults=1&key=AIzaSyDKckSI_0asveDz4lHuU_KMrmmU9zy0-18&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Channel not found: @gustavo-entrala\n",
      "No videos found for any of the specified channels\n"
     ]
    }
   ],
   "source": [
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "YOUTUBE_CHANNELS = [\"@la_inteligencia_artificial\", \"@dotcsv\", \"@gustavo-entrala\"]\n",
    "a = process_youtube_channels(YOUTUBE_API_KEY, YOUTUBE_CHANNELS, max_videos=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
